# Implementation Plan: Physical AI RAG Chatbot System

**Branch**: `001-physical-ai-rag-chatbot` | **Date**: 2025-12-06 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-physical-ai-rag-chatbot/spec.md`

## Summary

Build a production-ready RAG (Retrieval-Augmented Generation) chatbot system integrated into the Physical AI Docusaurus book. The system enables learners to ask natural language questions and receive accurate answers with citations, using a hybrid storage architecture (Neon Postgres + Qdrant) for optimal performance. Implementation follows the physical-ai-rag-builder skill's 7-agent orchestration strategy across 5 distinct phases.

## Technical Context

**Language/Version**: Python 3.11+ (backend), Node.js 18+ (frontend)
**Primary Dependencies**:
- Backend: FastAPI, OpenAI SDK, SQLAlchemy, Qdrant Client, Alembic
- Frontend: Docusaurus 3.x, React 18, TypeScript 4.9+
**Storage**:
- Metadata: PostgreSQL 15+ with pgvector extension (Neon Serverless Postgres for production)
- Vectors: Qdrant (local Docker for dev, Qdrant Cloud for production)
**Testing**: pytest (backend), Jest (frontend), Playwright (E2E)
**Target Platform**:
- Frontend: GitHub Pages (static site)
- Backend: Render (ASGI server with managed Postgres)
**Project Type**: Web (monorepo with frontend/ + backend/)
**Performance Goals**:
- p95 query response time <2s
- Support 10+ concurrent users
- Retrieval accuracy >85%
- Citation accuracy 100%
**Constraints**:
- No hardcoded secrets (environment variables only)
- CORS properly configured for GitHub Pages domain
- Mobile-responsive (viewport ≥320px)
- Graceful error handling (no stack traces to users)
**Scale/Scope**:
- ~1M tokens of book content
- ~10K embedded chunks
- 4 modules × ~10 chapters each
- Single-tenant (no multi-user auth required for v1)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### ✅ I. Content-First Documentation
- **Status**: PASS
- **Evidence**: Existing book content in docs/ with module structure (docs/module-01-ros2/, docs/module-02-gazebo-unity/, docs/module-03-nvidia-isaac/, docs/module-04-vla-conversational/)
- **Action**: Phase 0 will validate content structure and chunk boundaries

### ✅ II. Docusaurus Architecture
- **Status**: PASS
- **Evidence**: Docusaurus site already initialized with autogenerated sidebars (sidebars.ts)
- **Action**: Phase 2 (docusaurus-setup) will integrate ChatInterface component

### ✅ III. GitHub Pages Deployment
- **Status**: PASS (deferred to post-v1)
- **Evidence**: Plan includes GitHub Actions workflow for automated deployment
- **Action**: Deployment configuration in Phase 5 deliverables

### ✅ IV. RAG Chatbot Integration
- **Status**: PASS
- **Evidence**: Core requirement of this feature
- **Action**: Entire plan focused on RAG implementation with citation support

### ✅ V. Hybrid Storage Architecture
- **Status**: PASS
- **Evidence**: Plan specifies Neon Postgres (pgvector) + Qdrant Cloud
- **Action**: Phase 3 (hybrid-storage-setup) implements dual-write strategy

### ✅ VI. Modern AI Stack
- **Status**: PASS
- **Evidence**: FastAPI backend, OpenAI embeddings (text-embedding-3-small), GPT-4-turbo generation
- **Action**: Phase 3 (rag-pipeline-builder) implements streaming responses

### ✅ VII. Test-First for Critical Paths
- **Status**: PASS
- **Evidence**: Phase 5 (testing-validation) mandates >80% coverage before completion
- **Action**: TDD enforced for RAG pipeline, retrieval accuracy, API contracts

**Overall Gate Status**: ✅ PASS - All constitutional principles satisfied

## Project Structure

### Documentation (this feature)

```text
specs/001-physical-ai-rag-chatbot/
├── plan.md              # This file (/sp.plan command output)
├── research.md          # Phase 0 output (/sp.plan command)
├── data-model.md        # Phase 1 output (/sp.plan command)
├── quickstart.md        # Phase 1 output (/sp.plan command)
├── contracts/           # Phase 1 output (/sp.plan command)
│   ├── openapi.yaml     # API specification for backend
│   └── schemas/         # Pydantic schema definitions
└── tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

```text
# Monorepo structure for web application

frontend/                      # Docusaurus application
├── docs/                      # Book content (existing)
│   ├── module-01-ros2/
│   ├── module-02-gazebo-unity/
│   ├── module-03-nvidia-isaac/
│   └── module-04-vla-conversational/
├── src/
│   ├── components/
│   │   └── ChatInterface/     # RAG chatbot UI
│   │       ├── ChatInterface.tsx
│   │       ├── ChatMessage.tsx
│   │       └── ChatInterface.module.css
│   ├── services/
│   │   └── api.ts             # Backend API client
│   ├── pages/
│   └── theme/
├── static/
│   └── js/
│       └── api-config.js      # Environment-based API URL
├── docusaurus.config.ts
├── sidebars.ts
├── package.json
└── tsconfig.json

backend/                       # FastAPI application
├── app/
│   ├── __init__.py
│   ├── main.py                # FastAPI app entry
│   ├── api/
│   │   ├── __init__.py
│   │   ├── routes/
│   │   │   ├── __init__.py
│   │   │   ├── chat.py        # POST /api/v1/chat
│   │   │   ├── documents.py   # GET /api/v1/documents
│   │   │   └── health.py      # GET /api/v1/health
│   │   └── deps.py            # Dependency injection
│   ├── core/
│   │   ├── __init__.py
│   │   ├── config.py          # Settings (Pydantic BaseSettings)
│   │   └── security.py        # CORS, auth (future)
│   ├── models/
│   │   ├── __init__.py
│   │   ├── database.py        # SQLAlchemy models
│   │   └── schemas.py         # Pydantic request/response schemas
│   ├── services/
│   │   ├── __init__.py
│   │   ├── rag.py             # RAG pipeline orchestration
│   │   ├── embeddings.py      # OpenAI embeddings generation
│   │   ├── retrieval.py       # Hybrid search (vector + BM25)
│   │   └── storage.py         # Database operations
│   └── utils/
│       ├── __init__.py
│       ├── chunking.py        # Document chunking logic
│       └── logging.py         # Structured logging setup
├── migrations/                # Alembic migrations
│   ├── versions/
│   └── env.py
├── tests/
│   ├── __init__.py
│   ├── conftest.py            # pytest fixtures
│   ├── test_api/              # API endpoint tests
│   │   ├── test_chat.py
│   │   ├── test_health.py
│   │   └── test_documents.py
│   └── test_services/         # Service layer tests
│       ├── test_rag.py
│       ├── test_embeddings.py
│       ├── test_retrieval.py
│       └── test_storage.py
├── scripts/
│   ├── ingest_documents.py    # Data ingestion pipeline
│   ├── init_db.py             # Database initialization
│   └── validate_setup.py      # System health validation
├── requirements.txt
├── requirements-dev.txt
└── pyproject.toml

shared/                        # Shared configurations
├── docker/
│   ├── docker-compose.yml     # Local development stack
│   ├── Dockerfile.backend
│   └── Dockerfile.frontend
└── config/
    └── .env.example           # Template for environment variables

.github/                       # CI/CD workflows
└── workflows/
    ├── backend-tests.yml      # Backend test automation
    ├── frontend-build.yml     # Frontend build validation
    └── deploy.yml             # Deployment pipeline (future)

Makefile                       # Development commands (install, dev, test, clean)
README.md                      # Setup and usage documentation
```

**Structure Decision**: Monorepo structure selected to enable unified development workflow, shared CI/CD pipeline, and atomic commits across frontend/backend. The `frontend/` directory contains the Docusaurus site (existing) with integrated ChatInterface component. The `backend/` directory contains the FastAPI RAG service. The `shared/` directory contains Docker Compose for local development. This structure aligns with the physical-ai-rag-builder skill's recommended project-structure.md pattern.

## Complexity Tracking

*No constitutional violations - all gates passed.*

---

## Phase 0: Research & Prerequisites

**Duration**: 1-2 days
**Status**: Pending
**Execution Strategy**: Use Task tool with general-purpose agent

### Research Questions to Resolve

1. **OpenAI Model Selection**: Compare text-embedding-3-small (1536 dim) vs text-embedding-3-large (3072 dim) for embedding quality and cost tradeoffs
2. **Chunking Strategy**: Determine optimal token sizes for theoretical content (proposal: 800), code blocks (proposal: 1200), and mixed content (proposal: 1000)
3. **Hybrid Search Weighting**: Research optimal ratio for vector vs BM25 scoring (proposal: 70% vector, 30% BM25)
4. **Neon Postgres Setup**: Validate pgvector extension availability and performance characteristics on Neon Serverless
5. **Qdrant Cloud Free Tier Limits**: Confirm 1GB storage and 100K vector limits sufficient for ~10K chunks (1536-dim embeddings)
6. **CORS Configuration**: Research best practices for GitHub Pages + Render backend CORS setup
7. **Docusaurus Chat Widget Patterns**: Investigate floating widget implementation patterns compatible with Docusaurus theme

### Deliverables

- **research.md**: Consolidated findings with decisions, rationale, alternatives considered
- **ADR Candidates Identified**:
  - ADR-001: OpenAI Model Selection (embedding + generation models)
  - ADR-002: Hybrid Storage Architecture (Postgres + Qdrant strategy)
  - ADR-003: RAG Pipeline Design (chunking, retrieval, generation flow)

### Success Criteria

- All "NEEDS CLARIFICATION" items from Technical Context resolved
- Technology choices justified with evidence (benchmarks, documentation, cost analysis)
- No blocking unknowns remaining for Phase 1 design

---

## Phase 1: Architecture & Planning

**Duration**: 2-3 days
**Agent**: backend-architecture (Plan agent from physical-ai-rag-builder skill)
**Status**: Pending
**Prerequisites**: Phase 0 research.md complete

### Agent Task Description

```
Design a FastAPI backend architecture for a RAG chatbot serving the Physical AI book.

Requirements:
- RESTful API with /api/v1/chat endpoint (POST with streaming support)
- Hybrid storage: Neon Postgres (metadata + pgvector) + Qdrant (vector search)
- OpenAI embeddings (text-embedding-3-small, 1536 dimensions)
- Pydantic schemas for request/response validation
- CORS configuration for GitHub Pages frontend (https://<username>.github.io/<repo>)
- Health check and analytics endpoints

Reference: .claude/skills/physical-ai-rag-builder/references/rag-architecture.md

Produce:
1. API endpoint specifications (OpenAPI/Swagger schema in specs/001-physical-ai-rag-chatbot/contracts/openapi.yaml)
2. Database schema (ERD + SQLAlchemy models design in specs/001-physical-ai-rag-chatbot/data-model.md)
3. Service layer architecture (RAG pipeline flowchart)
4. Configuration management approach (.env structure)
5. Testing strategy (unit, integration, E2E test requirements)
```

### Deliverables

1. **data-model.md**: Entity-relationship diagram with tables:
   - `documents`: Book pages/sections (title, module, chapter, section, content_type, file_path, metadata)
   - `chunks`: Embedded content pieces (content, chunk_index, token_count, vector_id, metadata)
   - `user_queries`: Query log (query_text, query_type, response_time, retrieved_chunks)
   - `chunk_usage`: Retrieval analytics (chunk_id, query_id, rank, score, used_in_context)

2. **contracts/openapi.yaml**: API specification with endpoints:
   - `POST /api/v1/chat`: Chat request/response with streaming support
   - `GET /api/v1/health`: Health check with database connection status
   - `GET /api/v1/documents`: List documents with filtering

3. **contracts/schemas/**: Pydantic schema definitions:
   - `ChatRequest`: query (str, max 500 chars)
   - `ChatResponse`: response (str), sources (list of citations)
   - `Citation`: module, chapter, section, url_fragment
   - `HealthResponse`: status (healthy/unhealthy), services (dict)

4. **quickstart.md**: Development setup guide:
   - Prerequisites (Docker, Docker Compose, Python 3.11+, Node.js 18+)
   - Environment variables (.env setup)
   - Database initialization (migrations)
   - Running the development stack (`make dev`)
   - Testing (`make test`)

5. **Service Layer Design**: RAG pipeline architecture diagram showing:
   - Query → Embedding Generation (OpenAI API)
   - Embedding → Hybrid Search (Qdrant vector + Postgres BM25)
   - Retrieved Chunks → Context Assembly (deduplication, ranking)
   - Context + Query → LLM Generation (OpenAI GPT-4-turbo)
   - Response → Citation Extraction (map chunks to book sections)

### Technical Decisions

1. **OpenAI Model Selection**:
   - Embeddings: text-embedding-3-small (1536 dim) - cost-effective, sufficient quality
   - Generation: GPT-4-turbo - best reasoning for educational content

2. **Chunking Strategy**:
   - Theoretical content: 800 tokens (dense concepts, smaller chunks for precision)
   - Code blocks: 1200 tokens (preserve complete functions/examples)
   - Mixed content: 1000 tokens (default balance)
   - 200 token overlap between chunks (context preservation)

3. **Hybrid Search Weighting**:
   - Vector search: 70% (semantic understanding)
   - BM25 keyword search: 30% (exact term matching)
   - Adaptive weighting based on query classification

4. **Caching Strategy**:
   - Embedding cache: Redis or in-memory LRU (80% cache hit rate target)
   - Query cache: Disabled (educational context requires fresh responses)

5. **Rate Limiting**:
   - Not implemented for v1 (educational platform, good faith usage)
   - Monitoring in place to detect anomalies

### Validation Checklist

- [ ] API endpoints documented with request/response schemas (OpenAPI spec)
- [ ] Database schema includes all required tables with relationships
- [ ] RAG pipeline flow clearly defined with component responsibilities
- [ ] CORS and security considerations addressed (allowed origins, input validation)
- [ ] Error handling strategy defined (graceful degradation, user-friendly messages)
- [ ] Configuration management approach documented (.env variables)
- [ ] Testing strategy covers critical paths (RAG accuracy, API contracts, database operations)

---

## Phase 2: Foundation Setup

**Duration**: 3-4 days
**Agents**: docusaurus-setup + project-structure-organizer (Parallel execution)
**Status**: Pending
**Prerequisites**: Phase 1 architecture complete

### Agent 1: docusaurus-setup (General-Purpose)

**Task Description**:
```
Set up Docusaurus site for Physical AI book with integrated chat interface.

Context: Docusaurus site already exists with content in docs/ directory.

Tasks:
1. Verify existing Docusaurus configuration (docusaurus.config.ts, sidebars.ts)
2. Create ChatInterface component using template (.claude/skills/physical-ai-rag-builder/assets/ChatInterface.tsx.template)
3. Implement API client service layer (src/services/api.ts) with TypeScript interfaces
4. Add custom CSS for floating chat widget (button trigger + expandable overlay)
5. Configure API URL via customFields in docusaurus.config.ts (support dev/prod)
6. Test component rendering and basic interactions

Reference: .claude/skills/physical-ai-rag-builder/assets/ChatInterface.tsx.template

Produce:
- src/components/ChatInterface/ChatInterface.tsx (React component)
- src/components/ChatInterface/ChatInterface.module.css (styles)
- src/services/api.ts (API client with chatWithRAG() function)
- static/js/api-config.js (environment-based API URL configuration)
- Updated docusaurus.config.ts (customFields.apiUrl)
```

**Deliverables**:
- ChatInterface component (floating widget with trigger button + chat panel)
- API client service (TypeScript interfaces, error handling, retry logic)
- Custom CSS (responsive design, mobile-friendly)
- Environment configuration (dev: localhost:8000, prod: render.com)

**Validation**:
- [ ] Docusaurus site runs successfully (`npm start`)
- [ ] ChatInterface component renders without errors
- [ ] Chat widget can be opened/closed
- [ ] API client configured with correct backend URL (dev vs prod)
- [ ] Build succeeds (`npm run build`)

### Agent 2: project-structure-organizer (General-Purpose)

**Task Description**:
```
Organize the complete project structure for Physical AI RAG system.

Reference: .claude/skills/physical-ai-rag-builder/references/project-structure.md

Tasks:
1. Create backend directory structure:
   - app/api/routes/ (chat.py, documents.py, health.py)
   - app/services/ (rag.py, embeddings.py, retrieval.py, storage.py)
   - app/models/ (database.py, schemas.py)
   - app/core/ (config.py, security.py)
   - tests/, scripts/, migrations/

2. Create shared directory structure:
   - shared/docker/ (docker-compose.yml, Dockerfiles)
   - shared/config/ (.env.example)

3. Create development tooling:
   - Makefile (install, dev, test, build, clean commands)
   - .gitignore (Python + Node.js + environment files)
   - README.md (setup instructions, architecture overview)

4. Create placeholder files with docstrings (no implementation yet)

Validation: Run scripts/validate_setup.py to verify structure

Produce:
- Complete directory structure (backend/, shared/)
- Makefile with unified commands
- Docker Compose configuration (Postgres + Qdrant + backend + frontend)
- .env.example with all required variables
- README.md with setup instructions
```

**Deliverables**:
- Backend directory structure (all subdirectories with __init__.py files)
- Docker Compose configuration (4 services: postgres, qdrant, backend, frontend)
- Makefile with commands: `make install`, `make dev`, `make test`, `make clean`
- .env.example documenting all required environment variables
- README.md with quickstart guide

**Validation**:
- [ ] Monorepo structure created correctly (frontend/, backend/, shared/)
- [ ] Backend directory structure matches architecture design
- [ ] Frontend directory structure from docusaurus-setup preserved
- [ ] Makefile commands work (`make install` succeeds)
- [ ] Docker Compose configuration valid (docker-compose config passes)
- [ ] No file conflicts between frontend/backend
- [ ] scripts/validate_setup.py passes (structure validation only, no runtime checks yet)

---

## Phase 3: Backend Implementation

**Duration**: 5-7 days
**Agents**: hybrid-storage-setup → rag-pipeline-builder (Sequential execution)
**Status**: Pending
**Prerequisites**: Phase 2 foundation complete

### Agent 1: hybrid-storage-setup (General-Purpose)

**Task Description**:
```
Set up hybrid storage system with Neon Postgres and Qdrant.

Reference: .claude/skills/physical-ai-rag-builder/references/hybrid-storage.md

Phase 1: Neon Postgres Setup
1. Implement SQLAlchemy models (documents, chunks, user_queries, chunk_usage)
2. Create Alembic migrations (initial schema + indexes)
3. Add database triggers:
   - Auto-update tsvector for full-text search (on insert/update)
   - Auto-update updated_at timestamps
4. Configure connection pooling (asyncpg with pool size limits)
5. Create initialization script (scripts/init_db.py)

Phase 2: Qdrant Setup
1. Create Qdrant collection (physical_ai_chunks)
2. Configure vector params (size=1536, distance=COSINE)
3. Set up payload indexes (module, chunk_type, learning_level)
4. Implement connection client in app/services/storage.py

Phase 3: Integration
5. Implement synchronization strategy (dual-write with unique vector_id)
6. Create ingestion pipeline (scripts/ingest_documents.py):
   - Parse Markdown/MDX from docs/ directory
   - Chunk content (variable sizes based on type)
   - Generate embeddings (OpenAI API)
   - Insert into Postgres + Qdrant atomically
7. Add reconciliation job for consistency checks
8. Implement hybrid query patterns (vector + BM25)

Test:
- Run migrations successfully
- Create Qdrant collection
- Ingest sample document (docs/module-01-ros2/01-fundamentals.md)
- Query both databases successfully (hybrid search returns results)

Produce:
- app/models/database.py (SQLAlchemy models with relationships)
- migrations/versions/*.py (Alembic migrations)
- app/services/storage.py (database operations, Qdrant client)
- scripts/init_db.py (database + collection initialization)
- scripts/ingest_documents.py (full ingestion pipeline)
```

**Deliverables**:
- SQLAlchemy models (documents, chunks, user_queries, chunk_usage)
- Alembic migrations (schema creation, indexes, triggers)
- Qdrant collection configuration (vector params, payload indexes)
- Storage service (database operations, Qdrant client)
- Ingestion pipeline (document parsing, chunking, embedding, dual-write)
- Initialization script (create database, run migrations, create collection)

**Validation**:
- [ ] Database migrations run successfully (alembic upgrade head)
- [ ] All tables created with correct schema (documents, chunks, user_queries, chunk_usage)
- [ ] Triggers created (tsvector auto-update, updated_at auto-update)
- [ ] Qdrant collection created and accessible (verify with Qdrant dashboard)
- [ ] Payload indexes configured (module, chunk_type, learning_level)
- [ ] Connection pooling works (asyncpg pool initialization)
- [ ] Sample document ingested successfully (at least 1 book chapter)
- [ ] Hybrid query returns results (vector search + BM25 both functional)

### Agent 2: rag-pipeline-builder (General-Purpose)

**Task Description**:
```
Implement the complete RAG pipeline for Physical AI book queries.

Reference: .claude/skills/physical-ai-rag-builder/references/rag-architecture.md

Phase 1: Embeddings Service (app/services/embeddings.py)
1. OpenAI client configuration (async client, API key from env)
2. Batch embedding generation (handle rate limits, retries)
3. Error handling and retries (exponential backoff)
4. Caching strategy (in-memory LRU or Redis)

Phase 2: Retrieval Service (app/services/retrieval.py)
5. Vector search in Qdrant (with metadata filters: module, chunk_type, learning_level)
6. BM25 search in Postgres (full-text search with ts_rank)
7. Hybrid scoring and reranking (70% vector + 30% BM25, normalize scores)
8. Query classification (conceptual, code, troubleshooting, comparison)
9. Metadata filtering (apply user preferences, module focus)

Phase 3: RAG Service (app/services/rag.py)
10. Query processing and embedding (validate input, generate query embedding)
11. Context retrieval (hybrid search, retrieve top 6-8 chunks)
12. Context assembly and deduplication (remove duplicates, merge overlapping chunks)
13. Prompt construction with Physical AI context (system prompt + retrieved context)
14. Response generation with OpenAI (GPT-4-turbo, streaming support)
15. Citation extraction and formatting (map chunks to module/chapter/section)

Phase 4: Utilities (app/utils/chunking.py)
16. Document chunking (variable sizes: 800/1200/1000 tokens based on type)
17. Metadata extraction (module, chapter, section from file path and frontmatter)
18. Preprocessing and cleaning (remove artifacts, normalize whitespace)

Test:
- Generate embeddings for sample queries
- Retrieve relevant chunks (verify top results are relevant)
- Generate complete response with citations
- Verify citation accuracy (links point to correct book sections)

Produce:
- app/services/embeddings.py (OpenAI embedding generation with caching)
- app/services/retrieval.py (hybrid search implementation)
- app/services/rag.py (end-to-end RAG pipeline)
- app/utils/chunking.py (document chunking logic)
- Unit tests in tests/test_services/ (test_embeddings.py, test_retrieval.py, test_rag.py)
```

**Deliverables**:
- Embeddings service (OpenAI client, batch processing, caching)
- Retrieval service (hybrid search: vector + BM25, query classification, reranking)
- RAG service (end-to-end pipeline: query → retrieval → generation → citations)
- Chunking utilities (variable token sizes, metadata extraction)
- Unit tests (>80% coverage for services)

**Validation**:
- [ ] OpenAI embeddings generation works (test with sample text)
- [ ] Hybrid search returns relevant results (vector + BM25 combined)
- [ ] Query classification accurate (conceptual vs code vs troubleshooting)
- [ ] Context assembly produces coherent context (no duplicates, proper ordering)
- [ ] Response generation includes citations (module, chapter, section)
- [ ] Citations link to correct book sections (URL fragments valid)
- [ ] Error handling for API failures (OpenAI rate limits, network errors)
- [ ] Unit tests pass (pytest tests/test_services/)

---

## Phase 4: Integration

**Duration**: 3-4 days
**Agent**: integration-layer (General-Purpose)
**Status**: Pending
**Prerequisites**: Phase 3 backend complete

### Task Description

```
Integrate the Docusaurus frontend with FastAPI backend.

Reference:
- .claude/skills/physical-ai-rag-builder/references/project-structure.md
- .claude/skills/physical-ai-rag-builder/assets/main.py.template

Backend Tasks (FastAPI):
1. Implement API routes in app/api/routes/chat.py:
   - POST /api/v1/chat (main chat endpoint with streaming)
   - GET /api/v1/health (health check with database status)
   - GET /api/v1/documents (list documents with filtering)

2. Wire up RAG service to chat endpoint:
   - Parse ChatRequest (validate query length, sanitize input)
   - Call rag_service.generate_response()
   - Format ChatResponse with sources (citations)
   - Support streaming responses (SSE or WebSocket)

3. Configure CORS in app/main.py:
   - Allow frontend origin (http://localhost:3000 for dev, https://<username>.github.io for prod)
   - Allow credentials (cookies, authorization headers)
   - Set proper headers (Content-Type, Accept)

4. Add error handling middleware:
   - Validation errors (422 with field details)
   - Server errors (500 with user-friendly message)
   - Rate limiting (429, future)

Frontend Tasks (Docusaurus):
5. Implement API client in src/services/api.ts:
   - chatWithRAG() function (POST to /api/v1/chat)
   - Error handling and retries (exponential backoff)
   - TypeScript interfaces (ChatRequest, ChatResponse, Citation)

6. Connect ChatInterface to API client:
   - Call API on message send
   - Handle loading states (spinner, progress indicator)
   - Display errors gracefully ("I encountered an error. Please try again.")
   - Show sources with citations (clickable links to book sections)

7. Environment configuration:
   - Use window.API_CONFIG for API URL (static/js/api-config.js)
   - Support dev and prod environments (localhost vs render.com)
   - No hardcoded URLs in components

Integration Test:
8. Start both servers (`make dev`)
9. Send chat message from frontend
10. Verify backend receives request (check logs)
11. Verify response displayed correctly in chat
12. Verify sources shown with clickable links

Produce:
- backend/app/api/routes/chat.py (complete implementation)
- backend/app/main.py (CORS configured, middleware added)
- frontend/src/services/api.ts (complete implementation)
- frontend/src/components/ChatInterface/ChatInterface.tsx (connected to API)
- Integration test demonstrating end-to-end flow (manual test script)
```

### Deliverables

1. **Backend API Routes**:
   - `chat.py`: POST /api/v1/chat with streaming support
   - `health.py`: GET /api/v1/health with database connection checks
   - `documents.py`: GET /api/v1/documents with filtering

2. **FastAPI Main App**:
   - CORS configuration (allowed origins from env)
   - Error handling middleware (validation errors, server errors)
   - OpenAPI documentation (auto-generated at /docs)

3. **Frontend API Client**:
   - TypeScript interfaces (ChatRequest, ChatResponse, Citation)
   - chatWithRAG() function (async, error handling, retries)
   - Environment-based URL configuration

4. **ChatInterface Integration**:
   - API client integration (call on message send)
   - Loading states (spinner during request)
   - Error display (user-friendly messages)
   - Source rendering (citations with clickable links)

5. **Environment Configuration**:
   - static/js/api-config.js (window.API_CONFIG)
   - Support for dev (localhost:8000) and prod (render.com)

### Validation Checklist

- [ ] Backend API endpoints respond correctly (test with curl/Postman)
- [ ] CORS allows frontend requests (no preflight errors)
- [ ] Frontend can send chat requests (network tab shows successful POST)
- [ ] Responses display in chat interface (text renders correctly)
- [ ] Sources shown with module/chapter links (clickable citations)
- [ ] Errors handled gracefully (user-friendly messages, no stack traces)
- [ ] Loading states work (spinner shows during request)
- [ ] Both dev and prod configurations work (localhost and github.io origins)

---

## Phase 5: Testing & Validation

**Duration**: 2-3 days
**Agent**: testing-validation (General-Purpose)
**Status**: Pending
**Prerequisites**: Phase 4 integration complete

### Task Description

```
Comprehensive testing and validation of Physical AI RAG system.

Backend Testing:
1. Unit tests for services:
   - test_embeddings.py (embedding generation, caching)
   - test_retrieval.py (hybrid search, query classification)
   - test_rag.py (end-to-end pipeline, citation extraction)
   - test_storage.py (database operations, Qdrant queries)

2. Integration tests for API:
   - test_chat_endpoint.py (POST /api/v1/chat with various queries)
   - test_health_endpoint.py (GET /api/v1/health status checks)
   - test_error_handling.py (validation errors, server errors)

3. Performance tests:
   - Query latency (<2s p95 requirement)
   - Embedding generation time (batching efficiency)
   - Database query performance (Postgres + Qdrant)
   - Memory usage (reasonable limits)

Frontend Testing:
4. Component tests:
   - ChatInterface rendering (snapshot tests)
   - Message display (user vs assistant messages)
   - API error handling (network failures, timeouts)

5. E2E tests:
   - User sends message → receives response (full flow)
   - Sources displayed correctly (citations with links)
   - Error states shown (graceful degradation)

System Validation:
6. Run validation script: python scripts/validate_setup.py
7. Verify all components healthy (database connected, Qdrant accessible, API responding)
8. Test with sample queries from each category:
   - Conceptual: "What is ROS2?"
   - Code: "Show me a ROS2 publisher example"
   - Troubleshooting: "How to fix node not found error?"

9. Measure quality metrics:
   - Retrieval accuracy (relevant chunks retrieved, manual evaluation)
   - Citation accuracy (sources match content)
   - Response quality (coherent, helpful, grounded in book content)

10. Load testing (optional):
    - Concurrent user simulation (10+ simultaneous queries)
    - Database connection pooling (no connection exhaustion)
    - Response time under load (p95 still <2s)

Produce:
- Complete test suite in backend/tests/ (unit + integration)
- Test coverage report (>80% for services and API)
- Performance benchmark results (latency, throughput)
- Validation report with metrics (retrieval accuracy, citation accuracy)
- List of any issues found (with severity and resolution plan)
```

### Deliverables

1. **Backend Test Suite**:
   - Unit tests: test_embeddings.py, test_retrieval.py, test_rag.py, test_storage.py
   - Integration tests: test_chat_endpoint.py, test_health_endpoint.py, test_error_handling.py
   - Performance tests: test_performance.py (latency benchmarks)
   - Coverage report: >80% for services and API

2. **Frontend Test Suite**:
   - Component tests: ChatInterface.test.tsx (rendering, interactions)
   - API client tests: api.test.ts (error handling, retries)
   - E2E tests: chat-flow.test.ts (full user journey)

3. **System Validation**:
   - scripts/validate_setup.py (health checks for all components)
   - Sample query test results (conceptual, code, troubleshooting)
   - Quality metrics report (retrieval accuracy, citation accuracy, response quality)

4. **Performance Benchmarks**:
   - Query latency (p50, p95, p99)
   - Concurrent user load test results
   - Database query performance (Postgres, Qdrant)

5. **Documentation**:
   - Testing guide (how to run tests, interpret results)
   - Troubleshooting guide (common issues, solutions)
   - Quality metrics baseline (acceptable thresholds)

### Validation Checklist

- [ ] All unit tests pass (pytest backend/tests/test_services/)
- [ ] Integration tests pass (pytest backend/tests/test_api/)
- [ ] E2E tests demonstrate full flow (user query → response with citations)
- [ ] Performance meets targets (p95 response time <2s)
- [ ] Test coverage >80% (backend services and API)
- [ ] Validation script passes (all components healthy)
- [ ] Sample queries produce good results (accurate, relevant, cited)
- [ ] No critical issues found (or all critical issues resolved)

---

## Dependencies & Critical Path

```
Phase 0: Research (1-2 days)
         Research unknowns, document decisions
         ↓
Phase 1: Architecture (2-3 days)
         backend-architecture (Plan agent)
         API design, database schema, service architecture
         ↓
Phase 2: Foundation (3-4 days) [PARALLEL]
         ╔═════════════════╦═════════════════════╗
         ↓                 ↓                     ↓
    docusaurus-setup  project-structure   (wait for both)
    ChatInterface     organizer
    component         Monorepo structure
         ↓                 ↓                     ↓
         ╚═════════════════╩═════════════════════╝
                           ↓
Phase 3: Backend (5-7 days) [SEQUENTIAL]
         hybrid-storage-setup
         Postgres + Qdrant configuration
         ↓
         rag-pipeline-builder
         RAG services implementation
         ↓
Phase 4: Integration (3-4 days)
         integration-layer
         Connect frontend to backend
         ↓
Phase 5: Testing (2-3 days)
         testing-validation
         Comprehensive test suite, validation
```

**Total Duration**: 14-21 days (approximately 3-4 weeks)

**Critical Path**: Phase 0 → Phase 1 → Phase 2 (parallel) → Phase 3 (sequential) → Phase 4 → Phase 5

**Parallel Execution**: Only Phase 2 agents (docusaurus-setup + project-structure-organizer) can run in parallel. All other phases are sequential due to dependencies.

---

## Risk Analysis

### Risk 1: OpenAI API Limits
**Impact**: High (blocks core functionality)
**Probability**: Medium (depends on usage volume)
**Mitigation**:
- Implement rate limiting (respect OpenAI rate limits)
- Use embedding caching (reduce redundant API calls)
- Monitor usage (alerting for approaching limits)
- Fallback strategy (queue requests, retry with exponential backoff)

### Risk 2: Retrieval Accuracy Below Target (85%)
**Impact**: High (poor user experience)
**Probability**: Medium (RAG tuning required)
**Mitigation**:
- Iterate on chunking strategy (test different token sizes)
- Tune hybrid search weights (benchmark vector vs BM25 ratios)
- Implement reranking (cross-encoder for top results)
- Gather user feedback (implicit via follow-up query rate)

### Risk 3: CORS Configuration Issues
**Impact**: Medium (blocks frontend-backend communication)
**Probability**: Low (well-documented pattern)
**Mitigation**:
- Test CORS early (Phase 4 integration testing)
- Use environment-based origins (avoid hardcoding)
- Document allowed origins clearly (.env.example)
- Test with actual GitHub Pages deployment

### Risk 4: Database Performance Degradation
**Impact**: Medium (affects response time)
**Probability**: Low (proper indexing mitigates)
**Mitigation**:
- Add database indexes (module, chunk_type, tsvector)
- Monitor query times (structured logging)
- Use connection pooling (asyncpg with limits)
- Optimize queries (EXPLAIN ANALYZE for slow queries)

### Risk 5: Content Volume Exceeds Embedding Budget
**Impact**: Low (cost increase, not functionality loss)
**Probability**: Low (~1M tokens within budget)
**Mitigation**:
- Start with 1 module (incremental ingestion)
- Monitor embedding costs (track API usage)
- Implement selective re-embedding (only changed content)
- Consider cheaper embedding models if needed (text-embedding-3-small)

---

## Milestones

### M1: Planning Complete (End of Week 1)
**Date**: Day 5
**Deliverables**:
- research.md with all decisions documented
- data-model.md with database schema
- contracts/openapi.yaml with API specification
- quickstart.md with setup guide

**Success Criteria**:
- All research questions resolved
- Architecture reviewed and approved
- No blocking unknowns for implementation

### M2: Foundation Set Up (End of Week 2)
**Date**: Day 10
**Deliverables**:
- ChatInterface component integrated
- Monorepo structure created
- Docker Compose configuration working
- Makefile commands functional

**Success Criteria**:
- `make dev` starts all services
- Docusaurus site accessible at localhost:3000
- Backend health endpoint responding
- No structural issues

### M3: Backend Complete (End of Week 3)
**Date**: Day 17
**Deliverables**:
- Hybrid storage configured (Postgres + Qdrant)
- RAG pipeline implemented
- Sample data ingested
- API endpoints functional

**Success Criteria**:
- End-to-end RAG query works (query → retrieval → generation → citations)
- Database migrations run successfully
- Unit tests pass for services

### M4: Integration & Testing Complete (End of Week 4)
**Date**: Day 21
**Deliverables**:
- Frontend-backend integration working
- Test suite complete (>80% coverage)
- Validation script passing
- Documentation complete

**Success Criteria**:
- User can ask question and get response with citations
- Response time <2s (p95)
- All tests pass
- scripts/validate_setup.py reports healthy

---

## Success Criteria (Final)

### Functional Requirements

- [ ] FR-001: Chat interface accessible from every page (floating widget)
- [ ] FR-002: Accepts natural language questions (up to 500 characters)
- [ ] FR-003: Classifies queries by type (conceptual, code, troubleshooting, comparison)
- [ ] FR-004: Retrieves relevant content via hybrid search (vector + keyword)
- [ ] FR-005: Generates responses based solely on book content (no hallucinations)
- [ ] FR-006: Includes citations in every response (module, chapter, section)
- [ ] FR-007: Citations are clickable links to book sections (URL fragments)
- [ ] FR-008: Chunks content intelligently (800/1200/1000 tokens by type)
- [ ] FR-009: Stores metadata in Postgres (relational structure)
- [ ] FR-010: Stores embeddings in Qdrant (vector search)
- [ ] FR-011: Synchronizes metadata and vectors (unique IDs)
- [ ] FR-012: Health check endpoint available (/api/v1/health)
- [ ] FR-013: Document metadata endpoint available (/api/v1/documents)
- [ ] FR-014: Validates API inputs (Pydantic schemas)
- [ ] FR-015: Handles errors gracefully (user-friendly messages)
- [ ] FR-016: Logs queries with timestamps (analytics, no PII)
- [ ] FR-017: Tracks chunk usage (retrieval analytics)
- [ ] FR-018: Supports existing book structure (4 modules)
- [ ] FR-019: Responsive interface (mobile, tablet, desktop)
- [ ] FR-020: Displays responses in real-time (streaming or progress)
- [ ] FR-021: Deduplicates retrieved chunks (no repetition)
- [ ] FR-022: Caches embeddings (avoid reprocessing)
- [ ] FR-023: Environment-based configuration (no hardcoded secrets)
- [ ] FR-024: Runs locally via Docker (containerized development)
- [ ] FR-025: Single command to start all services (`make dev`)

### Performance Targets

- [ ] SC-001: Response time <2s (p95 latency)
- [ ] SC-002: Retrieval accuracy ≥85% (manually evaluated)
- [ ] SC-003: Citation accuracy 100% (all responses cited)
- [ ] SC-004: Supports 10+ concurrent users (no degradation)
- [ ] SC-005: Mobile-responsive (viewport ≥320px)
- [ ] SC-006: 90% queries answered on first attempt (low follow-up rate)
- [ ] SC-007: Test coverage ≥80% (frontend + backend)
- [ ] SC-008: All services start with `make dev` (<30s)
- [ ] SC-009: Zero critical security vulnerabilities (no hardcoded secrets, proper CORS)
- [ ] SC-010: Validation script reports healthy (all components)

### Agent Completion

- [ ] All 7 agents completed successfully:
  1. ✅ backend-architecture (Plan)
  2. ✅ docusaurus-setup (General-purpose)
  3. ✅ project-structure-organizer (General-purpose)
  4. ✅ hybrid-storage-setup (General-purpose)
  5. ✅ rag-pipeline-builder (General-purpose)
  6. ✅ integration-layer (General-purpose)
  7. ✅ testing-validation (General-purpose)

### System Validation

- [ ] scripts/validate_setup.py passes (all health checks)
- [ ] Sample queries produce accurate results:
  - Conceptual: "What is ROS2?" → relevant explanation with citations
  - Code: "Show me a ROS2 publisher example" → code snippet with context
  - Troubleshooting: "How to fix node not found error?" → solution from book
- [ ] Documentation complete (README, quickstart, API docs, troubleshooting)

---

## Reference Documentation

### Primary References (physical-ai-rag-builder skill)

- **SKILL.md**: Agent orchestration strategy, task descriptions, validation checklists
- **references/rag-architecture.md**: RAG pipeline patterns, chunking, retrieval, generation
- **references/hybrid-storage.md**: Database schemas, Qdrant config, query patterns
- **references/project-structure.md**: Directory structure, integration patterns, deployment
- **assets/ChatInterface.tsx.template**: Frontend component template
- **assets/main.py.template**: Backend FastAPI structure (if available)
- **assets/docker-compose.yml.template**: Docker configuration (if available)

### Secondary References

- Feature specification: specs/001-physical-ai-rag-chatbot/spec.md
- Constitution: .specify/memory/constitution.md
- Project instructions: CLAUDE.md

---

## Next Steps After Completion

1. **Content Ingestion**: Ingest all Physical AI book chapters (all 4 modules)
2. **Fine-tuning**: Adjust chunking sizes, retrieval weights, prompt templates based on user feedback
3. **Analytics**: Add usage tracking dashboard, popular queries analysis, feedback collection
4. **Optimization**:
   - Cache frequent queries (Redis)
   - Optimize database indexes (query performance)
   - Implement query result caching
5. **Deployment**:
   - Deploy frontend to GitHub Pages
   - Deploy backend to Render (with managed Postgres)
   - Configure Qdrant Cloud production instance
6. **Monitoring**:
   - Set up structured logging (JSON format)
   - Add error tracking (Sentry)
   - Configure uptime monitoring (health check pings)

---

## Appendix: Agent Task Commands

### Phase 0: Research
```bash
# Use Task tool with general-purpose agent
Task(
  subagent_type="general-purpose",
  description="Research RAG system decisions",
  prompt="Research questions from specs/001-physical-ai-rag-chatbot/plan.md Phase 0. Produce research.md with findings."
)
```

### Phase 1: Architecture
```bash
# Use Task tool with Plan agent
Task(
  subagent_type="Plan",
  description="Design backend architecture",
  prompt="[Full task description from Phase 1 section above]"
)
```

### Phase 2: Foundation (Parallel)
```bash
# Launch both agents in parallel (single message, multiple tool calls)
Task(subagent_type="general-purpose", description="Setup Docusaurus", prompt="[docusaurus-setup task]")
Task(subagent_type="general-purpose", description="Organize project structure", prompt="[project-structure-organizer task]")
```

### Phase 3: Backend (Sequential)
```bash
# Agent 1: Storage
Task(
  subagent_type="general-purpose",
  description="Setup hybrid storage",
  prompt="[hybrid-storage-setup task]"
)

# Wait for completion, then Agent 2: RAG Pipeline
Task(
  subagent_type="general-purpose",
  description="Build RAG pipeline",
  prompt="[rag-pipeline-builder task]"
)
```

### Phase 4: Integration
```bash
Task(
  subagent_type="general-purpose",
  description="Integrate frontend and backend",
  prompt="[integration-layer task]"
)
```

### Phase 5: Testing
```bash
Task(
  subagent_type="general-purpose",
  description="Test and validate system",
  prompt="[testing-validation task]"
)
```

---

**Plan Status**: Complete - Ready for execution
**Next Command**: `/sp.tasks` (generate task breakdown from this plan)
